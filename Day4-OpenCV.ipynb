{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV on Images\n",
    "\n",
    "[openCV website](https://opencv.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#import numpy as np\n",
    "\n",
    "img = cv2.imread(\"images/lena.jpg\",1)\n",
    "\n",
    "ret, th = cv2.threshold(img, 125, 255, cv2.THRESH_BINARY)\n",
    "ret, th1 = cv2.threshold(img, 125, 255, cv2.THRESH_BINARY_INV)\n",
    "ret, th2 = cv2.threshold(img, 125, 255, cv2.THRESH_TOZERO)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Hello World1\",img)\n",
    "cv2.imshow(\"Hello World2\",th)\n",
    "cv2.imshow(\"Hello World3\",th1)\n",
    "cv2.imshow(\"Hello World4\",th2)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "Height of the image:  512 pixels\n",
      "Width of the image:  512 pixels\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"images/lena.jpg\")\n",
    "\n",
    "cv2.imshow(\"Hello World\",img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "print(\"Height of the image: \", int(img.shape[0]), \"pixels\")\n",
    "print(\"Width of the image: \", int(img.shape[1]), \"pixels\")\n",
    "\n",
    "cv2.imwrite(\"Output.jpg\",img)\n",
    "cv2.imwrite(\"Output.png\",img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos\n",
    "\n",
    "code to take input from camera and save the result in current working directory\n",
    "\n",
    "train the photos taken from camera on with [app](https://teachablemachine.withgoogle.com/train/image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opencv_frame_0.png written!\n",
      "opencv_frame_1.png written!\n",
      "opencv_frame_2.png written!\n",
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"test\")\n",
    "img_counter = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow(\"test\", frame)\n",
    "    if not ret:\n",
    "        break\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenCV Example files](https://github.com/Itseez/opencv/tree/master/data/haarcascades)\n",
    "\n",
    " Face and eye detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the Haar cascades for face and eye detection\n",
    "face_cascade = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('xml/haarcascade_eye.xml')\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, img = cap.read()\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw rectangles around the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect eyes within the face region of interest (ROI)\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('img', img)\n",
    "    \n",
    "    # Break the loop if 'ESC' key is pressed\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the Haar cascades for face, eye, and eyeglasses detection\n",
    "face_cascade = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('xml/haarcascade_eye.xml')\n",
    "glasses_cascade = cv2.CascadeClassifier('xml/haarcascade_eye_tree_eyeglasses.xml')  # Path to your eyeglasses XML file\n",
    "\n",
    "if face_cascade.empty() or eye_cascade.empty() or glasses_cascade.empty():\n",
    "    raise IOError(\"One or more Haar cascade xml files not found.\")\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam.\")\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw rectangles around the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect eyes within the face region of interest (ROI)\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "\n",
    "        # Detect eyeglasses within the face region of interest (ROI)\n",
    "        glasses = glasses_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        for (gx, gy, gw, gh) in glasses:\n",
    "            cv2.rectangle(roi_color, (gx, gy), (gx+gw, gy+gh), (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('img', img)\n",
    "    \n",
    "    # Break the loop if 'ESC' key is pressed\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smile detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the Haar cascades for face and smile detection\n",
    "face_cascade = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('xml/haarcascade_smile.xml')\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, img = cap.read()\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw rectangles around the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect smiles within the face region of interest (ROI)\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(roi_color, (sx, sy), (sx+sw, sy+sh), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('img', img)\n",
    "    \n",
    "    # Break the loop if 'ESC' key is pressed\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from [google object translate](https://thing-translator.appspot.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colleting Samples Complete!!!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "def face_extractor(img):\n",
    "\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    for(x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count+=1\n",
    "        face = cv2.resize(face_extractor(frame),(200,200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        file_name_path = '/images/output/sample/FR'+ str(count) +'.jpg'\n",
    "        cv2.imwrite(file_name_path,face)\n",
    "\n",
    "        cv2.putText(face,str(count),(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "        cv2.imshow('Face Cropper',face)\n",
    "    else:\n",
    "        print(\"Face not Found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1)==13 or count==100:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Colleting Samples Complete!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Complete!!!!!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "data_path = 'images/output/'  # Use forward slashes for cross-platform compatibility\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "for i, file_name in enumerate(onlyfiles):\n",
    "    image_path = join(data_path, file_name)  # Correct path concatenation\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is not None:\n",
    "        Training_Data.append(image)  # Append the image as is, no need to convert\n",
    "        Labels.append(i)\n",
    "    else:\n",
    "        print(f\"Failed to load image {image_path}\")\n",
    "\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "Training_Data = np.asarray(Training_Data, dtype=np.uint8)  # Convert list to numpy array of uint8\n",
    "\n",
    "# Ensure that opencv-contrib-python is installed for cv2.face.LBPHFaceRecognizer_create\n",
    "if cv2.__version__.startswith('4.'):\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "else:\n",
    "    raise ImportError(\"OpenCV 4.x or later is required for cv2.face.LBPHFaceRecognizer_create.\")\n",
    "\n",
    "model.train(Training_Data, Labels)\n",
    "\n",
    "print(\"Model Training Complete!!!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Samples Complete!!!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the face classifier\n",
    "face_classifier = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Define relative path\n",
    "relative_path = 'images/output/sample/'\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(relative_path, exist_ok=True)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count += 1\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Define the file name path using a relative path\n",
    "        file_name_path = os.path.join(relative_path, f'FR{count}.jpg')\n",
    "        cv2.imwrite(file_name_path, face)\n",
    "\n",
    "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "    else:\n",
    "        print(\"Face not Found\")\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13 or count == 100:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('Collecting Samples Complete!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Complete!!!!!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Load training data\n",
    "data_path = 'images/output/sample/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "Training_Data, Labels = [], []\n",
    "\n",
    "for i, file in enumerate(onlyfiles):\n",
    "    image_path = join(data_path, file)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is not None:\n",
    "        Training_Data.append(np.asarray(image, dtype=np.uint8))\n",
    "        Labels.append(i)\n",
    "\n",
    "Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "# Create and train the model\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "\n",
    "print(\"Model Training Complete!!!!!\")\n",
    "\n",
    "# Load face classifier\n",
    "face_classifier = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return img, None\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "        return img, roi\n",
    "\n",
    "    return img, None\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    image, face = face_detector(frame)\n",
    "\n",
    "    if face is not None:\n",
    "        face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        result = model.predict(face_gray)\n",
    "\n",
    "        if result[1] < 500:\n",
    "            confidence = int(100 * (1 - (result[1]) / 300))\n",
    "            display_string = str(confidence) + '% Confidence it is user'\n",
    "        else:\n",
    "            display_string = \"Low Confidence\"\n",
    "\n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (250, 120, 255), 2)\n",
    "\n",
    "        if confidence > 75:\n",
    "            cv2.putText(image, \"Unlocked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2)\n",
    "    else:\n",
    "        cv2.putText(image, \"Face Not Found\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Face Cropper', image)\n",
    "\n",
    "    if cv2.waitKey(1) == 13:  # Break the loop if 'Enter' key is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Sketch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def sketch(frame):\n",
    "\t'''\n",
    "\tGenerate sketch given an image\n",
    "\t@paramaters: frame \n",
    "\t'''\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tgray_blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "\tedges = cv2.Canny(gray_blur, 10, 70)\n",
    "\tret, mask = cv2.threshold(edges, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\treturn mask\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "\tresponse, frame = capture.read()\n",
    "\tcv2.imshow(\"Those edges\", sketch(frame))\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def sketch(frame):\n",
    "\t'''\n",
    "\tGenerate sketch given an image\n",
    "\t@paramaters: frame \n",
    "\t'''\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tblur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\tcanny = cv2.Canny(blur, 10, 70)\n",
    "\t#lap = cv2.Laplacian(blur, cv2.CV_8UC1)\n",
    "\n",
    "\t# Adaptive Thresholding - No need to spicify threshold value\n",
    "\tthresh = cv2.adaptiveThreshold(canny, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "\t                               cv2.THRESH_BINARY, 3, 5)\n",
    "\treturn thresh\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "\tresponse, frame = capture.read()\n",
    "\tcv2.imshow(\"Those edges(Adaptive Thresholding)\", sketch(frame))\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "capture.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smile detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import cv2\n",
    "\n",
    "# Loading the cascades\n",
    "face_cascade = cv2.CascadeClassifier('xml/haarcascade_frontalface_default.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('xml/haarcascade_smile.xml')\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    response, frame = capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 250, 0), 2)\n",
    "        cropped_gray = gray[y:y+h, x:x+w]\n",
    "        cropped_color = frame[y:y+h, x:x+w]\n",
    "        smiles = smile_cascade.detectMultiScale(cropped_gray, 1.6, 22)\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(cropped_color, (sx, sy), (sx+sw, sy+sh), (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Smile Detector', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
